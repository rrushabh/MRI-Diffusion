{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yxTgWx7avRs",
    "outputId": "11658f75-fb81-4bf9-fe7f-1b3e2b475827"
   },
   "outputs": [],
   "source": [
    "!pip install monai\n",
    "!pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMyJvJ8bzskx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import monai\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, ResizeWithPadOrCropd, NormalizeIntensityd, ToTensor\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAw34T8rtqTb"
   },
   "outputs": [],
   "source": [
    "num_decoders = 8\n",
    "decoder_depth = 5\n",
    "decoder_channels = 32\n",
    "img_dim = 64\n",
    "img_channels = 5\n",
    "batch_size = 16\n",
    "corruption_channels = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /scratch/ras10116/shared/cvproject/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxH1R1PP91qW"
   },
   "source": [
    "# Dataloader with transforms after slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ9myLIg-6S4"
   },
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the path to the data\n",
    "data_path = \"\" # TODO This path needs to point to an actual data source\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, transforms, slice_range=(40, 76)):\n",
    "        self.file_paths = file_paths\n",
    "        self.transforms = transforms\n",
    "        self.slice_range = slice_range\n",
    "        self.images = self._load_images()\n",
    "        self.slice_sets = self._create_slice_sets()\n",
    "\n",
    "    def _load_images(self):\n",
    "        loader = LoadImaged(keys=['T2'])\n",
    "        images = []\n",
    "        for file_path in self.file_paths:\n",
    "            data = loader({'T2': file_path})\n",
    "            images.append(data['T2'])\n",
    "        return images\n",
    "\n",
    "    def _create_slice_sets(self):\n",
    "        slice_sets = []\n",
    "        for i, _ in enumerate(self.images):\n",
    "            for slice_start in range(*self.slice_range):\n",
    "                slice_sets.append((i, slice_start))\n",
    "        return slice_sets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_sets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx, slice_start = self.slice_sets[idx]\n",
    "        sequential_slices = self.images[image_idx][:, :, slice_start:slice_start + 5]\n",
    "        transformed_slices = self.transforms({'T2': sequential_slices})['T2']\n",
    "        return transformed_slices.squeeze().permute(2, 0, 1)\n",
    "\n",
    "# Define transformations, excluding LoadImaged\n",
    "transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=['T2']),\n",
    "    ResizeWithPadOrCropd(keys=['T2'], spatial_size=(64, 64, 5)),\n",
    "    NormalizeIntensityd(keys=['T2'], nonzero=True, channel_wise=True),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T1aBup_A7xC"
   },
   "source": [
    "# load images at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z7I4beps4Jrv",
    "outputId": "7994e57f-a48d-49bd-e2e3-c34ad042e6dc"
   },
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "file_paths = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.nii.gz')]\n",
    "dataset = CustomDataset(file_paths, transforms)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "num_train = len(dataset)\n",
    "split = int(num_train * 0.2)\n",
    "train_set, valid_set = random_split(dataset, [num_train - split, split])\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a batch for demonstration\n",
    "train_loader_iterator = iter(train_loader)\n",
    "batch = next(train_loader_iterator)\n",
    "\n",
    "print(batch.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "  # Example: Get the first sample of 5 slices\n",
    "  first_item = batch[0][0]\n",
    "\n",
    "  # Now you can display the image\n",
    "  plt.imshow(first_item.squeeze(), cmap='gray')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqjTI77tzsk8"
   },
   "outputs": [],
   "source": [
    "class normal_distribute_block(nn.Module):\n",
    "    def __init__(self, img_dim, sd, cc = []):\n",
    "        super().__init__()\n",
    "        self.img_dim = img_dim\n",
    "        self.sd = sd\n",
    "        self.corruption_channels = cc\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    def up_sd(self):\n",
    "      self.sd = self.sd + 0.05\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial_deviations = torch.ones(x.shape).to(device)\n",
    "        perturbation = self.sd * torch.randn((self.img_dim, self.img_dim)).to(device)\n",
    "        for i in self.corruption_channels:\n",
    "            initial_deviations[:, i, :, :] = perturbation + torch.ones((self.img_dim, self.img_dim)).to(device)\n",
    "        initial_mean = torch.zeros((self.img_dim, self.img_dim)).to(device)\n",
    "        initial_x = x\n",
    "        x = initial_deviations * x\n",
    "        x = x + initial_mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkXgJGlvzsk_"
   },
   "outputs": [],
   "source": [
    "class diffusion_forward(nn.Module):\n",
    "    def __init__(self, img_dim, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformation = nn.ModuleList([\n",
    "            normal_distribute_block(img_dim, 0.05, corruption_channels) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.img_dim = img_dim\n",
    "        self.rm_rec = 0\n",
    "        self.rm_sq = 0\n",
    "\n",
    "    def inc_diff(self):\n",
    "      for block in self.transformation:\n",
    "        block.up_sd()\n",
    "\n",
    "    def up_removed_box(self):\n",
    "      self.rm_sq = self.rm_sq + 1\n",
    "      self.rm_rec = self.rm_rec + 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0).repeat(self.num_layers+1, 1, 1, 1, 1)\n",
    "        for i, blur in enumerate(self.transformation):\n",
    "            x[i+1] = blur(x[i])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSvdqIB5zslA"
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, img_dim, num_layers, in_channels, initial_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolution_list = []\n",
    "        self.upscale_list = []\n",
    "        self.num_layers = num_layers\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.output_conv = nn.Conv2d(initial_channels, img_channels, 1) \n",
    "\n",
    "        self.convolution_list = nn.ModuleList([])\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convolution_list.append(nn.ModuleList([nn.Conv2d(in_channels, initial_channels, 3, padding=1), nn.Conv2d(initial_channels, initial_channels, 3, padding=1), nn.Conv2d(initial_channels * 2, initial_channels, 3, padding=1)]))\n",
    "            in_channels = initial_channels\n",
    "            initial_channels = initial_channels * 2\n",
    "\n",
    "        self.intermediate_conv = nn.Conv2d(in_channels, initial_channels, 3, padding = 1)\n",
    "        self.middle_conv = nn.Conv2d(initial_channels, initial_channels, 3, padding = 1)\n",
    "\n",
    "    def upscale(self, x):\n",
    "        x = x.repeat_interleave(2, dim=2)\n",
    "        x = x.repeat_interleave(2, dim=3)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs_list = []\n",
    "        for layer in range(self.num_layers):\n",
    "            x = self.convolution_list[layer][0](x)\n",
    "            for i in range(1, 3):\n",
    "                x = self.relu(self.convolution_list[layer][1](x))\n",
    "            xs_list.append(x)\n",
    "            x = self.max_pool(x)\n",
    "\n",
    "        x = self.intermediate_conv(x)\n",
    "        x = self.middle_conv(x)\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            backward_layer = self.num_layers - layer - 1\n",
    "            x = self.upscale(x)\n",
    "            x = self.convolution_list[backward_layer][2](x)\n",
    "            x = x + xs_list[backward_layer]\n",
    "            for i in range(1, 3):\n",
    "                x = self.convolution_list[backward_layer][1](x)\n",
    "\n",
    "        x = self.output_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqqZDfsHyAKK"
   },
   "outputs": [],
   "source": [
    "class diffusion_backward(nn.Module):\n",
    "  def __init__(self, img_dim, num_layers, in_channels, initial_channels, num_decoders):\n",
    "      super().__init__()\n",
    "\n",
    "      self.num_decoders = num_decoders\n",
    "      self.unets = nn.ModuleList([\n",
    "          Unet(img_dim, num_layers, in_channels, initial_channels) for i in range(num_decoders)\n",
    "      ])\n",
    "\n",
    "  def forward(self, x):\n",
    "      x_record = x.clone().unsqueeze(0).repeat(self.num_decoders+1, 1, 1, 1, 1)\n",
    "      for i in range(len(self.unets)):\n",
    "          x = self.unets[i](x.clone().detach())\n",
    "          x_record[num_decoders - i - 1] = x\n",
    "\n",
    "      return x_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CuMNrYYzslC"
   },
   "outputs": [],
   "source": [
    "# Dataset preparation with transformations\n",
    "dataset = CustomDataset(file_paths, transforms)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "num_train = len(dataset)\n",
    "split = int(num_train * 0.2)\n",
    "train_set, valid_set = random_split(dataset, [num_train - split, split])\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_set, batch_size= batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l8ibLtwBh4j_",
    "outputId": "2cfb406e-b7c6-43ad-87d2-d53d7b83b196"
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch for demonstration\n",
    "train_loader_iterator = iter(train_loader)\n",
    "\n",
    "batch = next(train_loader_iterator)\n",
    "print(batch.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "for batch_idx in range(4):\n",
    "  image = batch[batch_idx][2]\n",
    "  plt.imshow(image, cmap='gray')  # Display the third slice of the images --> this is the target\n",
    "  plt.show()\n",
    "\n",
    "print(\"All samples: \", num_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37TgdUshjuTf",
    "outputId": "b97b91dd-3490-4dca-e7c9-ba6f673b5e38"
   },
   "outputs": [],
   "source": [
    "model1 = diffusion_forward(img_dim, num_decoders).to(device)\n",
    "model2 = diffusion_backward(img_dim, decoder_depth, img_channels, decoder_channels, num_decoders).cuda()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "model2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "x0Azxg6QnzXV",
    "outputId": "418c34b0-e193-4e06-bce8-718e6ed5cbb7"
   },
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "for i in range(len(model2.unets)):\n",
    "    optimizers.append(torch.optim.Adam(model2.unets[num_decoders - i - 1].parameters(), lr=1, eps=1))\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "schedulers = []\n",
    "for i in range(len(model2.unets)):\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizers[i], step_size=50, gamma=0.5)\n",
    "\n",
    "cumulative_loss = 0\n",
    "for level in range(8):\n",
    "    model1.inc_diff()\n",
    "    if level % 2 == 0:\n",
    "        model1.up_removed_box()\n",
    "    for i in range(1, 500):\n",
    "        cumulative_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            images = data.to(device)\n",
    "            corrupted = model1(images)\n",
    "            restored = model2(corrupted[num_decoders])\n",
    "            total_loss = loss(restored, corrupted)\n",
    "\n",
    "            for j in range(num_decoders):\n",
    "                optimizers[j].zero_grad()\n",
    "\n",
    "            cumulative_loss += total_loss.item()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model2.parameters(), 0.1)\n",
    "            \n",
    "            for j in range(num_decoders):\n",
    "                optimizers[j].step()\n",
    "\n",
    "\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        \n",
    "        print(f'Difficulty Level: {level} , Epoch no: {i} , Loss: {cumulative_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzO2Yz-8QsGC",
    "outputId": "b712988d-7a5a-4a05-ea0c-460d0d09a44c"
   },
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "# Validation step\n",
    "model2.eval()  # Set the model to evaluation mode\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_idx, data in enumerate(valid_loader):  # Assume you have a validation_loader defined\n",
    "        images = data.to(device)\n",
    "        corrupted = model1(images)\n",
    "        restored = model2(corrupted[num_decoders])\n",
    "        loss_val = loss(restored[:,:,2:3,:,:], corrupted[:, :, 2:3, :, :])  # Use the appropriate loss function\n",
    "        validation_loss += loss_val.item()  # Sum up batch loss\n",
    "\n",
    "validation_loss /= len(valid_loader.dataset)  # Get the average loss over the entire validation dataset\n",
    "print(f'Epoch: 1: Validation Loss: {validation_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwQclp-vzslG"
   },
   "outputs": [],
   "source": [
    "batch = next(train_loader_iterator)\n",
    "print(batch.shape)\n",
    "images = batch.to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "corrupted = model1(images)\n",
    "restored = model2(corrupted[num_decoders].clone())\n",
    "print(restored.shape)\n",
    "image = images[0, 2].cpu().detach()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "image = corrupted[-1, 0, 2].cpu().detach()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "image = restored[0, 0, 2].cpu().detach()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(nn.MSELoss()(corrupted[-1, 0, 2], images[0, 2]).item())\n",
    "print(nn.MSELoss()(restored[0, 0, 2], images[0, 2]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"diffusion_model.pth\")\n",
    "torch.save(model2.state_dict(), file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
